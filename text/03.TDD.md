---
title: "Test driven data science"
author: "JJ Merelo"
date: "27/5/2022"
output: html_document
bibliography: ../mlops.bib
---

# Test driven development in data science

## TL;DR

Test-driven development, or simply development that uses extensive testing, is
essential to ensure quality in all phases of a data science pipeline, from data
extraction and munging to results. This chapter is an introduction to testing
and how to profitably use it to save time and enhance adaptability. Before
testing, we'll need to interiorize best practices in project and model design.

## Testing in ETL workflows

Machine learning starts with the ETL phase: Extract, Transform and Load where
you need to grab the data that's going to be used to train and test your
model. In this phase, we are going to need already a test-driven approach to
ascertain that all phases (extraction, transformation) occur according to our
preconceptions.

In general, this is a part of the process that benchmark-based ML tutorials
do not pay much attention to. ETL is reduced to loading your data in a Python
data frame. However, in real-world ML ETL is the most important part, and also a
part of the process that needs to be heavily automated so that it works as
intended and as long as possible.

What we will do is to explain test-driven development along with ETL techniques,
so that you start to familiarize yourself with them and put them to work later
in the rest of the phases of ML.

## Extracting information for machine learning

Data is Out There. Generally, in a way that's not too easy to obtain. It might
be in one of these forms.

* Non-processable formats, like PDF. Yes, it's non-processable. Don't even
  try. You might get lucky and get some numbers, but that's that. Tabular data
  is going to be hard, tabular data in fancy tables even harder. Just forget it.
* Websites in semi-structured form. This is going to be found generally in press
  releases, or even mandatory information. This can be obtained via scraping,
  that is, extracting information according to the web structure or the kind of
  words or symbols that surround it. Scraping, however, is an arcane art and
  you'd better allot it quite an amount of time, because it involves many
  tricks, even more so if the site does not like to be scraped (happens a lot in
  administration/government sites). We'll devote a bit of time to this one,
  later on.
* Processable formats: CSV and others. Sometimes you get lucky and find the
  precise data in open data portals. You will still need to transform it, but if
  you're lucky it will always have the same URL and it will be easy to extract
  by simply downloading.
* APIs: application interfaces are the best; in some cases open data portals
  will offer them, in others, like social networks or GitHub, they will offer
  you loads of data in easily accessible way. You will need to obtain a token,
  and there will be some limitations when downloading, but other than that,
  offers a semantic interface. The main issue here will be to respect terms of
  service regarding storing/republishing, as well as controlling the request or
  other (like download size) limits.

In all cases, you will need testing; in the case of scraping, you *need*
testing. Let's devote some time to that one.

## See also

Despite its title, Thoughtful Machine Learning is a great test-driven approach
to data science [@kirk2014thoughtful]

## References
